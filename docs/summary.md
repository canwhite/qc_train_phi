# ğŸ“ ä»é›¶å¼€å§‹çš„å¤§æ¨¡å‹è®­ç»ƒå®Œå…¨æŒ‡å—

ä½œä¸ºå°ç™½ï¼Œæˆ‘æ¥ç»™ä½ è¯¦ç»†è®²è§£æ¯ä¸ªæ­¥éª¤çš„ä½œç”¨å’Œç”¨åˆ°çš„å·¥å…·ï¼Œä¿è¯è®©ä½ å½»åº•æ˜ç™½ï¼

## ğŸ—ï¸ æ•´ä½“æ¶æ„ç†è§£

å…ˆæƒ³è±¡æˆ‘ä»¬è¦å»ºä¸€åº§æˆ¿å­ï¼š
- **æ¨¡å‹** = æˆ¿å­çš„åœ°åŸºå’Œç»“æ„
- **æ•°æ®** = å»ºé€ æˆ¿å­çš„ææ–™  
- **è®­ç»ƒ** = æ–½å·¥è¿‡ç¨‹
- **å·¥å…·** = æ–½å·¥è®¾å¤‡å’Œå·¥äºº

## ğŸ“‹ ç¯å¢ƒå‡†å¤‡é˜¶æ®µ

### 1. **å¯¼å…¥å·¥å…·åº“**
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
from peft import LoraConfig
from trl import SFTTrainer
```

**å·¥å…·è¯´æ˜**ï¼š
- `torch`ï¼šPyTorchï¼Œæ·±åº¦å­¦ä¹ çš„"å‘åŠ¨æœº"
- `transformers`ï¼šHugging Faceçš„æ ¸å¿ƒåº“ï¼Œæä¾›ç°æˆçš„æ¨¡å‹
- `datasets`ï¼šæ•°æ®å¤„ç†å·¥å…·ï¼Œåƒ"æ•°æ®æ¬è¿å·¥"
- `peft`ï¼šé«˜æ•ˆå¾®è°ƒå·¥å…·ï¼Œè®©è®­ç»ƒæ›´çœèµ„æº
- `trl`ï¼šè®­ç»ƒä¼˜åŒ–åº“ï¼Œæä¾›æ›´å¥½çš„è®­ç»ƒæ–¹æ³•

## ğŸ§© åˆ†æ­¥è¯¦ç»†è®²è§£

### æ­¥éª¤1ï¼šè®¾ç½®æ—¥å¿—
```python
def setup_logging():
    import logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.INFO,
    )
```
**ä½œç”¨**ï¼šå®‰è£…"ç›‘æ§æ‘„åƒå¤´"ï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•å’ŒæŸ¥çœ‹è¿›åº¦ã€‚

### æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
```python
def load_model_and_tokenizer():
    # åŠ è½½åˆ†è¯å™¨ - åƒ"ç¿»è¯‘å®˜"
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-mini-instruct")
    
    # åŠ è½½æ¨¡å‹ - åƒ"å¤§è„‘"
    model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3.5-mini-instruct")
    
    return model, tokenizer
```

**è¯¦ç»†è§£é‡Š**ï¼š
- **åˆ†è¯å™¨ (Tokenizer)**ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—çš„å·¥å…·
  - æ¯”å¦‚ï¼š"ä½ å¥½" â†’ [123, 456]
  - è®¡ç®—æœºåªè®¤è¯†æ•°å­—ï¼Œä¸è®¤è¯†æ–‡å­—

- **æ¨¡å‹ (Model)**ï¼šå·²ç»é¢„è®­ç»ƒå¥½çš„AIå¤§è„‘
  - å°±åƒå·²ç»ä¸Šè¿‡å¤§å­¦çš„èªæ˜å­¦ç”Ÿ
  - æˆ‘ä»¬åªéœ€è¦æ•™å®ƒç‰¹å®šçŸ¥è¯†

### æ­¥éª¤3ï¼šå‡†å¤‡æ•°æ®
è¿™æ˜¯æœ€å¤æ‚ä½†æœ€é‡è¦çš„ä¸€æ­¥ï¼

#### 3.1 åˆ›å»ºç¤ºä¾‹æ•°æ®
```python
def create_sample_alpaca_data():
    alpaca_data = [
        {
            "instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "input": "",
            "output": "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)"
        }
    ]
    return alpaca_data
```

**æ•°æ®æ ¼å¼è¯´æ˜**ï¼š
- `instruction`ï¼šæŒ‡ä»¤ï¼Œå‘Šè¯‰æ¨¡å‹è¦åšä»€ä¹ˆ
- `input`ï¼šé¢å¤–çš„è¾“å…¥ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
- `output`ï¼šæœŸæœ›çš„æ­£ç¡®å›ç­”

#### 3.2 æ•°æ®æ ¼å¼è½¬æ¢
```python
def convert_alpaca_to_messages_format(alpaca_data):
    messages_data = []
    
    for example in alpaca_data:
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_content = example["instruction"]
        if example.get("input", "").strip():
            user_content += "\n" + example["input"]
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": example["output"]}
        ]
        
        messages_data.append({"messages": messages})
    
    return messages_data
```

**ä¸ºä»€ä¹ˆè¦è½¬æ¢æ ¼å¼ï¼Ÿ**
- åŸå§‹æ•°æ®ï¼š`instruction + input â†’ output`
- è½¬æ¢åï¼š`ç”¨æˆ·é—® â†’ AIç­”` çš„å¯¹è¯æ ¼å¼
- å› ä¸ºæ¨¡å‹æ›´æ“…é•¿ç†è§£å¯¹è¯

#### 3.3 åº”ç”¨èŠå¤©æ¨¡æ¿
```python
def apply_chat_template(example):
    example["text"] = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,  # ä¸ç«‹å³è½¬æˆæ•°å­—
        add_generation_prompt=False
    )
    return example
```

**ä½œç”¨**ï¼šæŠŠå¯¹è¯æ ¼å¼è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼ã€‚

### æ­¥éª¤4ï¼šé…ç½®è®­ç»ƒå‚æ•°
```python
def setup_training_config():
    # è®­ç»ƒå‚æ•°
    training_config = {
        "output_dir": "./phi3-5-mini-finetuned",  # ä¿å­˜ä½ç½®
        "per_device_train_batch_size": 2,         # æ¯æ¬¡å¤„ç†çš„æ ·æœ¬æ•°
        "learning_rate": 5.0e-05,                 # å­¦ä¹ é€Ÿåº¦
        "num_train_epochs": 3,                    # è®­ç»ƒè½®æ•°
        "bf16": True,                             # ä½¿ç”¨åŠç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
    }
    
    # LoRAé…ç½® - é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
    peft_config = {
        "r": 16,          # å¾®è°ƒå‚æ•°çš„æ•°é‡
        "lora_alpha": 32, # å¾®è°ƒå¼ºåº¦
        "target_modules": "all-linear",  # åœ¨å“ªäº›å±‚å¾®è°ƒ
    }
```

**å‚æ•°è¯¦è§£**ï¼š
- **batch_size**ï¼šä¸€æ¬¡çœ‹å¤šå°‘æ¡æ•°æ®ï¼Œè¶Šå¤§è®­ç»ƒè¶Šå¿«ä½†éœ€è¦æ›´å¤šå†…å­˜
- **learning_rate**ï¼šå­¦ä¹ é€Ÿåº¦ï¼Œå¤ªå¤§å®¹æ˜“"å­¦è¿‡å¤´"ï¼Œå¤ªå°å­¦ä¹ å¤ªæ…¢
- **epochs**ï¼šæŠŠæ•´ä¸ªæ•°æ®é›†çœ‹å¤šå°‘é
- **LoRA**ï¼šåªè®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¤§èŠ‚çœèµ„æº

### æ­¥éª¤5ï¼šåˆ›å»ºè®­ç»ƒå™¨
```python
def create_trainer(model, train_dataset, eval_dataset, tokenizer):
    trainer = SFTTrainer(
        model=model,              # è¦è®­ç»ƒçš„æ¨¡å‹
        train_dataset=train_dataset,    # è®­ç»ƒæ•°æ®
        eval_dataset=eval_dataset,      # éªŒè¯æ•°æ®
        tokenizer=tokenizer,      # åˆ†è¯å™¨
        max_seq_length=2048,      # æœ€å¤§æ–‡æœ¬é•¿åº¦
        packing=True,             # æ‰“åŒ…æ–‡æœ¬ï¼Œæé«˜æ•ˆç‡
    )
    return trainer
```

**è®­ç»ƒå™¨çš„ä½œç”¨**ï¼šåƒ"æ•™ç»ƒ"ï¼Œè´Ÿè´£æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è°ƒåº¦å’Œç®¡ç†ã€‚

### æ­¥éª¤6ï¼šå¼€å§‹è®­ç»ƒ
```python
def train_model():
    # 1. è®¾ç½®ç›‘æ§
    setup_logging()
    
    # 2. å‡†å¤‡æ¨¡å‹å’Œå·¥å…·
    model, tokenizer = load_model_and_tokenizer()
    
    # 3. å‡†å¤‡æ•°æ®
    train_dataset, eval_dataset = prepare_dataset(tokenizer)
    
    # 4. é…ç½®å‚æ•°
    training_args, peft_config = setup_training_config()
    
    # 5. åˆ›å»ºæ•™ç»ƒ
    trainer = create_trainer(model, train_dataset, eval_dataset, tokenizer)
    
    # 6. å¼€å§‹è®­ç»ƒï¼
    trainer.train()
    
    # 7. ä¿å­˜è®­ç»ƒæˆæœ
    trainer.save_model()
```

### æ­¥éª¤7ï¼šæµ‹è¯•æ¨¡å‹
```python
def test_trained_model():
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("./phi3-5-mini-finetuned")
    
    # æé—®
    question = "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—é˜¶ä¹˜"
    
    # ç”Ÿæˆå›ç­”
    messages = [{"role": "user", "content": question}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å›ç­”: {response}")
```

## ğŸ› ï¸ å·¥å…·é“¾æ€»ç»“

| å·¥å…· | ä½œç”¨ | æ¯”å–» |
|------|------|------|
| **PyTorch** | æ·±åº¦å­¦ä¹ æ¡†æ¶ | å»ºç­‘å·¥åœ° |
| **Transformers** | é¢„è®­ç»ƒæ¨¡å‹åº“ | é¢„åˆ¶æˆ¿å±‹ç»„ä»¶ |
| **Datasets** | æ•°æ®å¤„ç†å·¥å…· | ææ–™åŠ å·¥å‚ |
| **PEFT** | é«˜æ•ˆå¾®è°ƒ | èŠ‚èƒ½æ–½å·¥æŠ€æœ¯ |
| **TRL** | è®­ç»ƒä¼˜åŒ– | é«˜çº§æ–½å·¥æ–¹æ³• |

## ğŸ¯ è®­ç»ƒè¿‡ç¨‹æ¯”å–»

æŠŠè®­ç»ƒè¿‡ç¨‹æƒ³è±¡æˆ**æ•™å°å­¦ç”Ÿåšæ•°å­¦é¢˜**ï¼š

1. **å‡†å¤‡æ•™æ**ï¼ˆæ•°æ®å‡†å¤‡ï¼‰
   - æ•´ç†é¢˜ç›®å’Œç­”æ¡ˆï¼ˆæ•°æ®æ ¼å¼åŒ–ï¼‰
   - æŠŠé¢˜ç›®å†™è§„èŒƒï¼ˆåº”ç”¨æ¨¡æ¿ï¼‰

2. **æ‰¾è€å¸ˆ**ï¼ˆåŠ è½½æ¨¡å‹ï¼‰
   - è¯·ä¸€ä¸ªæ•°å­¦è€å¸ˆï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰
   - å‡†å¤‡æ•™å­¦å·¥å…·ï¼ˆåˆ†è¯å™¨ï¼‰

3. **åˆ¶å®šæ•™å­¦è®¡åˆ’**ï¼ˆè®­ç»ƒé…ç½®ï¼‰
   - æ¯å¤©æ•™å‡ é“é¢˜ï¼ˆbatch_sizeï¼‰
   - æ•™å­¦è¿›åº¦ï¼ˆlearning_rateï¼‰
   - æ•™å¤šå°‘å¤©ï¼ˆepochsï¼‰

4. **å¼€å§‹æ•™å­¦**ï¼ˆè®­ç»ƒï¼‰
   - è€å¸ˆçœ‹é¢˜æ€è€ƒï¼ˆå‰å‘ä¼ æ’­ï¼‰
   - æ£€æŸ¥ç­”æ¡ˆå¯¹é”™ï¼ˆè®¡ç®—æŸå¤±ï¼‰
   - è°ƒæ•´ç†è§£ï¼ˆåå‘ä¼ æ’­ï¼‰

5. **æœŸæœ«è€ƒè¯•**ï¼ˆæµ‹è¯•ï¼‰
   - å‡ºé¢˜æµ‹è¯•ï¼ˆæé—®ï¼‰
   - çœ‹å›ç­”è´¨é‡ï¼ˆè¯„ä¼°æ•ˆæœï¼‰

## ğŸ’¡ ç»™æ–°æ‰‹çš„å»ºè®®

1. **å…ˆä»ç¤ºä¾‹æ•°æ®å¼€å§‹**ï¼šç”¨æˆ‘æä¾›çš„ç¤ºä¾‹æ•°æ®è·‘é€šæµç¨‹
2. **ç†è§£æ¯ä¸ªå‚æ•°**ï¼šä¸è¦ç›²ç›®å¤åˆ¶ï¼Œç†è§£æ¯ä¸ªå‚æ•°çš„ä½œç”¨
3. **å°è§„æ¨¡å®éªŒ**ï¼šå…ˆç”¨å°‘é‡æ•°æ®æµ‹è¯•ï¼ŒæˆåŠŸåå†ç”¨å¤§é‡æ•°æ®
4. **è€å¿ƒè°ƒè¯•**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šé‡åˆ°å„ç§é—®é¢˜ï¼Œè€å¿ƒè§£å†³

ç°åœ¨ä½ åº”è¯¥å¯¹æ•´ä¸ªè®­ç»ƒæµç¨‹æœ‰äº†æ¸…æ™°çš„ç†è§£ï¼å¦‚æœè¿˜æœ‰å“ªä¸ªæ­¥éª¤ä¸æ˜ç™½ï¼Œè¯·éšæ—¶é—®æˆ‘ï¼Œæˆ‘ä¼šç”¨æ›´ç®€å•çš„æ–¹å¼è§£é‡Šã€‚