# åŸºäºPhi-3.5-mini-instructçš„å®Œæ•´è®­ç»ƒæµç¨‹

ä¸‹é¢æ˜¯ä¸€ä¸ªä»æ•°æ®è½¬æ¢åˆ°æœ€ç»ˆè®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ŒåŸºäºMicrosoft/Phi-3.5-mini-instructå’ŒHugging Faceç”Ÿæ€ç³»ç»Ÿï¼š

```python
import os
import json
import torch
import datasets
from datasets import load_dataset, Dataset
from peft import LoraConfig
import transformers
from trl import SFTTrainer
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments,
    BitsAndBytesConfig
)

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    import logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.INFO,
    )

def load_model_and_tokenizer(model_name="microsoft/Phi-3.5-mini-instruct"):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    
    # æ¨¡å‹é…ç½®
    model_kwargs = {
        "use_cache": False,
        "trust_remote_code": True,
        "attn_implementation": "flash_attention_2",
        "torch_dtype": torch.bfloat16,
        "device_map": "auto"fa
    }
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token
    tokenizer.padding_side = 'right'
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    
    return model, tokenizer

def create_sample_alpaca_data():
    """åˆ›å»ºç¤ºä¾‹Alpacaæ ¼å¼æ•°æ®"""
    
    alpaca_data = [
        {
            "instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "input": "",
            "output": "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)"
        },
        {
            "instruction": "å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡",
            "input": "The weather is really nice today, let's go for a walk.",
            "output": "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘ä»¬å‡ºå»æ•£æ­¥å§ã€‚"
        },
        {
            "instruction": "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "input": "",
            "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚"
        },
        {
            "instruction": "å†™ä¸€å°æ±‚èŒä¿¡",
            "input": "ç”³è¯·è½¯ä»¶å·¥ç¨‹å¸ˆèŒä½ï¼Œæœ‰3å¹´Pythonç»éªŒ",
            "output": "å°Šæ•¬çš„æ‹›è˜ç»ç†ï¼š\n\næˆ‘å†™ä¿¡ç”³è¯·è´µå…¬å¸çš„è½¯ä»¶å·¥ç¨‹å¸ˆèŒä½...\n\næ­¤è‡´\næ•¬ç¤¼"
        },
        {
            "instruction": "æ€»ç»“ä»¥ä¸‹æ–‡ç« çš„ä¸»è¦å†…å®¹",
            "input": "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚ä»åŒ»ç–—è¯Šæ–­åˆ°è‡ªåŠ¨é©¾é©¶ï¼ŒAIæŠ€æœ¯æ­£åœ¨å„ä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“...",
            "output": "æ–‡ç« ä¸»è¦è®¨è®ºäº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„è¡Œå„ä¸šçš„å¹¿æ³›åº”ç”¨å’Œæ·±è¿œå½±å“ã€‚"
        }
    ]
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    os.makedirs("./data", exist_ok=True)
    with open("./data/alpaca_sample.json", "w", encoding="utf-8") as f:
        json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
    
    return "./data/alpaca_sample.json"

def convert_alpaca_to_messages_format(data_path):
    """å°†Alpacaæ ¼å¼è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼"""
    
    # åŠ è½½æ•°æ®
    with open(data_path, "r", encoding="utf-8") as f:
        alpaca_data = json.load(f)
    
    messages_data = []
    
    for example in alpaca_data:
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_content = example["instruction"]
        if example.get("input", "").strip():
            user_content += "\n" + example["input"]
        
        # æ„å»ºæ¶ˆæ¯æ ¼å¼
        messages = [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": example["output"]}
        ]
        
        messages_data.append({"messages": messages})
    
    return messages_data

def prepare_dataset(tokenizer, data_path=None, use_sample_data=True):
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
    
    if use_sample_data:
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        data_path = create_sample_alpaca_data()
    
    # è½¬æ¢æ•°æ®æ ¼å¼
    messages_data = convert_alpaca_to_messages_format(data_path)
    raw_dataset = Dataset.from_list(messages_data)
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿
    def apply_chat_template(example):
        example["text"] = tokenizer.apply_chat_template(
            example["messages"],
            tokenize=False,
            add_generation_prompt=False
        )
        return example
    
    # å¤„ç†æ•°æ®é›†
    processed_dataset = raw_dataset.map(
        apply_chat_template,
        remove_columns=raw_dataset.column_names
    )
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if len(processed_dataset) > 1:
        dataset = processed_dataset.train_test_split(test_size=0.2, seed=42)
        train_dataset = dataset["train"]
        eval_dataset = dataset["test"]
    else:
        train_dataset = processed_dataset
        eval_dataset = None
    
    return train_dataset, eval_dataset

def setup_training_config():
    """è®¾ç½®è®­ç»ƒé…ç½®"""
    
    # è®­ç»ƒå‚æ•°
    training_config = {
        "output_dir": "./phi3-5-mini-finetuned",
        "overwrite_output_dir": True,
        "per_device_train_batch_size": 2,
        "per_device_eval_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 5.0e-05,
        "num_train_epochs": 3,
        "max_steps": -1,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "logging_steps": 10,
        "save_steps": 100,
        "eval_steps": 100,
        "save_total_limit": 2,
        "evaluation_strategy": "steps",
        "bf16": True,
        "remove_unused_columns": False,
        "gradient_checkpointing": True,
        "gradient_checkpointing_kwargs": {"use_reentrant": False},
        "report_to": "none",  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
    }
    
    # LoRAé…ç½®
    peft_config = {
        "r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "bias": "none",
        "task_type": "CAUSAL_LM",
        "target_modules": "all-linear",
    }
    
    return TrainingArguments(**training_config), LoraConfig(**peft_config)

def train_model():
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    
    print("ğŸš€ å¼€å§‹Phi-3.5-mini-instructå¾®è°ƒæµç¨‹...")
    
    # 1. è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # 2. åŠ è½½æ¨¡å‹å’Œtokenizer
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œtokenizer...")
    model, tokenizer = load_model_and_tokenizer()
    
    # 3. å‡†å¤‡æ•°æ®
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_dataset, eval_dataset = prepare_dataset(
        tokenizer, 
        use_sample_data=True  # è®¾ç½®ä¸ºFalseå¹¶ä½¿ç”¨data_pathå‚æ•°æ¥ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®
    )
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset:
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")
    
    # 4. è®¾ç½®è®­ç»ƒé…ç½®
    print("âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    training_args, peft_config = setup_training_config()
    
    # 5. åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ¯ åˆ›å»ºSFTTrainer...")
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        peft_config=peft_config,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        tokenizer=tokenizer,
        max_seq_length=2048,
        packing=True,
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    train_result = trainer.train()
    
    # 7. ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜è®­ç»ƒç»“æœ...")
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()
    
    # 8. ä¿å­˜æ¨¡å‹
    print("ğŸ’¿ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {training_args.output_dir}")
    
    return trainer

def test_trained_model(model_path, test_questions):
    """æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹"""
    
    print("\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹...")
    
    # åŠ è½½è®­ç»ƒåçš„æ¨¡å‹å’Œtokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    for question in test_questions:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "user", "content": question}
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # ç”Ÿæˆå›ç­”
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"\nğŸ¤” é—®é¢˜: {question}")
        print(f"ğŸ¤– å›ç­”: {response[len(text):]}")
        print("-" * 50)

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
    trainer = train_model()
    
    # æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹
    test_questions = [
        "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—é˜¶ä¹˜",
        "å°†'Sample text for translation'ç¿»è¯‘æˆä¸­æ–‡",
        "è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
    ]
    
    test_trained_model("./phi3-5-mini-finetuned", test_questions)
```

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
phi3-training/
â”œâ”€â”€ train_phi3.py              # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ alpaca_sample.json     # ç¤ºä¾‹æ•°æ®
â”‚   â””â”€â”€ your_custom_data.json  # ä½ çš„è‡ªå®šä¹‰æ•°æ®
â”œâ”€â”€ phi3-5-mini-finetuned/     # è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ tokenizer_config.json
â””â”€â”€ requirements.txt
```

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

åˆ›å»º`requirements.txt`æ–‡ä»¶ï¼š

```txt
torch>=2.0.0
transformers>=4.37.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.7.0
trl>=0.7.0
bitsandbytes>=0.41.0
flash-attn>=2.0.0
```

å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

## ğŸ¯ ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®

è¦ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®ï¼Œåªéœ€ï¼š

1. **å‡†å¤‡Alpacaæ ¼å¼æ•°æ®**ï¼š
```python
your_data = [
    {
        "instruction": "ä½ çš„æŒ‡ä»¤",
        "input": "å¯é€‰è¾“å…¥", 
        "output": "æœŸæœ›è¾“å‡º"
    },
    # æ›´å¤šæ•°æ®...
]
```

2. **ä¿®æ”¹æ•°æ®åŠ è½½**ï¼š
```python
# åœ¨train_modelå‡½æ•°ä¸­ä¿®æ”¹è¿™ä¸€è¡Œï¼š
train_dataset, eval_dataset = prepare_dataset(
    tokenizer, 
    data_path="path/to/your/data.json",  # ä½ çš„æ•°æ®è·¯å¾„
    use_sample_data=False  # ä¸ä½¿ç”¨ç¤ºä¾‹æ•°æ®
)
```

## âš¡ è®­ç»ƒä¼˜åŒ–å»ºè®®

1. **æ˜¾å­˜ä¼˜åŒ–**ï¼š
   - å‡å°`per_device_train_batch_size`
   - å¢åŠ `gradient_accumulation_steps`
   - ä½¿ç”¨`4-bit`é‡åŒ–ï¼ˆéœ€è¦ä¿®æ”¹æ¨¡å‹åŠ è½½ï¼‰

2. **è´¨é‡ä¼˜åŒ–**ï¼š
   - å¢åŠ è®­ç»ƒæ•°æ®é‡å’Œè´¨é‡
   - è°ƒæ•´å­¦ä¹ ç‡ï¼ˆé€šå¸¸åœ¨1e-5åˆ°5e-5ä¹‹é—´ï¼‰
   - å¢åŠ è®­ç»ƒè½®æ•°

3. **é€Ÿåº¦ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æ›´å¼ºå¤§çš„GPU
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå·²å¯ç”¨bf16ï¼‰

è¿™ä¸ªå®Œæ•´æµç¨‹æ¶µç›–äº†ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹æµ‹è¯•çš„æ‰€æœ‰æ­¥éª¤ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚